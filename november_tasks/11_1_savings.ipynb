{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb29f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,  FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import cloudpickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f4dc9",
   "metadata": {},
   "source": [
    "# Regression Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c27d478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "file_path = os.path.join(\"data\",\"processed_data.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)  \n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "X=df.drop(columns=['Response Time', 'Latency', 'Service Name_0', 'Service Name_1', 'Service Name_2', 'Service Name_3', 'Compliance', 'Class', 'WsRF'])\n",
    "Y=df['Response Time']\n",
    "\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(\n",
    "    X, Y, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_rest, y_rest, test_size=0.53, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a6d2804",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'regression_package'\n",
    "\n",
    "numeric_features = ['Availability', 'Throughput', 'Successability',\n",
    "                    'Reliability', 'Best Practices', 'Documentation']\n",
    "\n",
    "\n",
    "def winsorize(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    threshold = 3\n",
    "    Xw = X.copy()\n",
    "    for j in range(Xw.shape[1]):\n",
    "        col = Xw[:, j]\n",
    "        # IQR clip\n",
    "        Q1 = np.percentile(col, 25)\n",
    "        Q3 = np.percentile(col, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        col = np.clip(col, lower, upper)\n",
    "\n",
    "        # z-score threshold\n",
    "        m = np.mean(col)\n",
    "        s = np.std(col)\n",
    "        if s == 0:\n",
    "            Xw[:, j] = col\n",
    "            continue\n",
    "        z = (col - m) / s\n",
    "        med = np.median(col)\n",
    "        col = np.where(np.abs(z) < threshold, col, med)\n",
    "        Xw[:, j] = col\n",
    "    return Xw\n",
    "\n",
    "winsorizer = FunctionTransformer(winsorize, validate=False)\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"winsorizer\", winsorizer),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numeric_transformer, numeric_features)\n",
    "], remainder='drop')  \n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_rest,Y_rest)\n",
    "\n",
    "with open(os.path.join(save_path, 'pipeline.pkl'), 'wb') as f:\n",
    "    cloudpickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa789c5",
   "metadata": {},
   "source": [
    "# Classification part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c267cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'classification_package'\n",
    "\n",
    "file_path = os.path.join(\"qws1\",\"data.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "X = df.drop(columns='Class')\n",
    "Y = df['Class']\n",
    "\n",
    "X_rest, X_test, Y_rest, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42, stratify = Y)\n",
    "\n",
    "\n",
    "if X_rest is not None and X_test is not None:\n",
    "    joblib.dump(X_rest, os.path.join(save_path, 'X_rest.pkl'))\n",
    "    joblib.dump(X_test, os.path.join(save_path, 'X_test.pkl'))\n",
    "    \n",
    "if Y_rest is not None and Y_test is not None:\n",
    "    joblib.dump(Y_rest, os.path.join(save_path, 'Y_rest.pkl'))\n",
    "    joblib.dump(Y_test, os.path.join(save_path, 'Y_test.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e17d0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0_0' '0_1' '0_2' '0_3' '0_4' '0_5' '0_6' '0_7' '0_8']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asaraiva\\AppData\\Local\\Temp\\ipykernel_9404\\2685545869.py:73: FutureWarning: `get_feature_names` is deprecated in all of sklearn. Use `get_feature_names_out` instead.\n",
      "  feat_names = cats_to_numeric.named_steps['encoder'].get_feature_names()\n"
     ]
    }
   ],
   "source": [
    "# --- Features ---\n",
    "numeric_features = ['Availability', 'Throughput', 'Successability',\n",
    "                    'Reliability', 'Compliance', 'Documentation']\n",
    "categorical_features = ['Service Name']\n",
    "\n",
    "# --- Winsorizer (safer order: after imputation) ---\n",
    "def winsorize(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    threshold = 3\n",
    "    Xw = X.copy()\n",
    "    for j in range(Xw.shape[1]):\n",
    "        col = Xw[:, j]\n",
    "        # IQR clip\n",
    "        Q1 = np.percentile(col, 25)\n",
    "        Q3 = np.percentile(col, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        col = np.clip(col, lower, upper)\n",
    "\n",
    "        # z-score threshold\n",
    "        m = np.mean(col)\n",
    "        s = np.std(col)\n",
    "        if s == 0:\n",
    "            Xw[:, j] = col\n",
    "            continue\n",
    "        z = (col - m) / s\n",
    "        med = np.median(col)\n",
    "        col = np.where(np.abs(z) < threshold, col, med)\n",
    "        Xw[:, j] = col\n",
    "    return Xw\n",
    "\n",
    "winsorizer = FunctionTransformer(winsorize, validate=False)\n",
    "\n",
    "# --- Categorical pipeline (no scaling) ---\n",
    "cats_to_numeric = Pipeline(steps=[\n",
    "    (\"imputer_cat\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", BinaryEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "\n",
    "# --- ColumnTransformer: scale numeric only ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer_num\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"winsor\", winsorizer),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_features),\n",
    "        (\"cat\", cats_to_numeric, categorical_features)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.0\n",
    ")\n",
    "\n",
    "# --- Drop columns by index (after transformation) ---\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_indices=None):\n",
    "        self.drop_indices = drop_indices\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.drop_indices:\n",
    "            return np.delete(X, self.drop_indices, axis=1)\n",
    "        return X\n",
    "\n",
    "# Fit preprocessor to compute feature names and select indices to drop\n",
    "preprocessor.fit(X_rest)\n",
    "\n",
    "cats_to_numeric.fit(X_rest[categorical_features])\n",
    "feat_names = cats_to_numeric.named_steps['encoder'].get_feature_names()\n",
    "print(feat_names)\n",
    "suffixes_to_drop = ('_0', '_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8')\n",
    "\n",
    "\n",
    "num_count = len(numeric_features)\n",
    "drop_indices = [num_count + i for i, n in enumerate(feat_names) if n.endswith(suffixes_to_drop)]\n",
    "\n",
    "# Build final pipeline: preprocess â†’ drop selected columns\n",
    "pipeline_all = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"drop_cols\", ColumnDropper(drop_indices=drop_indices)),\n",
    "    (\"final_imputer\", SimpleImputer(strategy=\"median\"))  # or \"most_frequent\"\n",
    "])\n",
    "\n",
    "# Fit the pipeline on your reference/train data\n",
    "pipeline_all.fit(X_rest)\n",
    "\n",
    "# Save\n",
    "import cloudpickle, os\n",
    "save_path = 'classification_package'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "with open(os.path.join(save_path, 'preprocessor.pkl'), 'wb') as f:\n",
    "    cloudpickle.dump(pipeline_all, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
