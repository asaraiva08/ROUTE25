{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "972ad9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import cloudpickle\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    RocCurveDisplay,\n",
    "    precision_recall_curve,\n",
    "    PrecisionRecallDisplay,\n",
    ")\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6941e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "saved_folder = 'classification_package'\n",
    "\n",
    "MODEL_PATH = os.path.join(saved_folder,\"model.pkl\")\n",
    "PREPROCESSOR_PATH = os.path.join(saved_folder,\"preprocessor.pkl\")\n",
    "REFERENCE_DATA_PATH = os.path.join(saved_folder,\"X_rest.pkl\")  \n",
    "REFERENCE_TARGET_PATH = os.path.join(saved_folder,\"Y_rest.pkl\")  \n",
    "UNSEEN_DATA_PATH = os.path.join(saved_folder,\"X_test.pkl\")\n",
    "UNSEEN_TARGET_PATH = os.path.join(saved_folder,\"Y_test.pkl\")\n",
    "\n",
    "\n",
    "# Load\n",
    "with open(PREPROCESSOR_PATH, 'rb') as f:\n",
    "    preprocessor = cloudpickle.load(f)\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(\"Loading unseen data...\")\n",
    "X_test = joblib.load(UNSEEN_DATA_PATH)\n",
    "Y_test = joblib.load(UNSEEN_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing...\n",
      "Generating predictions...\n",
      "Basic metrics:\n",
      "Accuracy: 0.43636363636363634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBasic metrics:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(Y_test, y_pred))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mROC AUC:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPR AUC (Average Precision):\u001b[39m\u001b[33m\"\u001b[39m, average_precision_score(Y_test, y_prob))\n\u001b[32m     19\u001b[39m prec, rec, f1, _ = precision_recall_fscore_support(Y_test, y_pred, average=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, zero_division=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:679\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    672\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    673\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPartial AUC computation not available in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    674\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmulticlass setting, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmax_fpr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    675\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set to `None`, received `max_fpr=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    676\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstead\u001b[39m\u001b[33m\"\u001b[39m.format(max_fpr)\n\u001b[32m    677\u001b[39m         )\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multi_class == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmulti_class must be in (\u001b[39m\u001b[33m'\u001b[39m\u001b[33movo\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33movr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[32m    681\u001b[39m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[32m    682\u001b[39m     )\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "# ---------------- PREPROCESS ----------------\n",
    "\n",
    "print(\"Applying preprocessing...\")\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "# ---------------- PREDICT ----------------\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "y_prob = model.predict_proba(X_test_transformed)[:, 1]  # for ROC/PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9c16b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43636363636363634\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAccuracy:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(Y_test, y_pred))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mROC AUC:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweighted\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPR AUC (Average Precision):\u001b[39m\u001b[33m\"\u001b[39m, average_precision_score(Y_test, y_prob))\n\u001b[32m      5\u001b[39m prec, rec, f1, _ = precision_recall_fscore_support(Y_test, y_pred, average=\u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m, zero_division=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\asaraiva\\OneDrive - Capgemini\\Documents\\ROUTE25\\ROUTE25\\november_tasks\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:679\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    672\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    673\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPartial AUC computation not available in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    674\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmulticlass setting, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmax_fpr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    675\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m set to `None`, received `max_fpr=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    676\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstead\u001b[39m\u001b[33m\"\u001b[39m.format(max_fpr)\n\u001b[32m    677\u001b[39m         )\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m multi_class == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmulti_class must be in (\u001b[39m\u001b[33m'\u001b[39m\u001b[33movo\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33movr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[32m    681\u001b[39m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[32m    682\u001b[39m     )\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m y_type == \u001b[33m\"\u001b[39m\u001b[33mbinary\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(Y_test, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(Y_test, y_prob))\n",
    "print(\"PR AUC (Average Precision):\", average_precision_score(Y_test, y_prob))\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(Y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "print(f\"Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curves\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "RocCurveDisplay.from_predictions(Y_test, y_prob, name=\"ROC\", ax=ax[0])\n",
    "ax[0].set_title(\"ROC Curve\")\n",
    "PrecisionRecallDisplay.from_predictions(Y_test, y_prob, name=\"PR\", ax=ax[1])\n",
    "ax[1].set_title(\"Precision-Recall Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ea8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Checking drift...\")\n",
    "reference_df = pd.read_csv(REFERENCE_DATA_PATH)\n",
    "psi_results = {}\n",
    "ks_results = {}\n",
    "\n",
    "for col in X_test.columns:\n",
    "    ref = reference_df[col]\n",
    "    new = X_unseen[col]\n",
    "    # PSI (simplified)\n",
    "    bins = pd.qcut(ref, q=10, duplicates=\"drop\")\n",
    "    ref_dist = bins.value_counts(normalize=True)\n",
    "    new_bins = pd.cut(new, bins=bins.cat.categories)\n",
    "    new_dist = new_bins.value_counts(normalize=True)\n",
    "    psi = sum((new_dist - ref_dist) * np.log(new_dist / ref_dist))\n",
    "    psi_results[col] = psi\n",
    "    # KS\n",
    "    ks_stat, ks_p = ks_2samp(ref, new)\n",
    "    ks_results[col] = {\"ks_stat\": ks_stat, \"p_value\": ks_p}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
